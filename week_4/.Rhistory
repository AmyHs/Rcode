docs <- tm_map(docs,toSpace, "不")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "很")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "那")
docs <- tm_map(docs,toSpace, "而")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "就")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "因為")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "而")
docs3 <- tm_map(docs3,toSpace, "的")
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "吧")
inspect(docs3)
docs3<- tm_map(docs3, stripWhitespace)
inspect(docs3)
docs3<-tm_map(docs3,removeWords,stopwordsCN())
inspect(docs3)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3,removeWords,stopwordsCN())
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "的")
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "嗎")
inspect(docs3)
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "個")
inspect(docs3)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "的")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "因為")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "而")
docs3 <- tm_map(docs3,toSpace, "吧")
inspect(docs3)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "的")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "但")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "而")
docs3 <- tm_map(docs3,toSpace, "吧")
inspect(docs3)
#拿來做w4分析
docmat<-DocumentTermMatrix(docs3)
docmat
inspect(docmat)
#矩陣
docmat <- TermDocumentMatrix(docs3, control = list(wordLengths = c(2, Inf)))
inspect(docmat)
?TermDocumentMatrix
#wordcloud
library(wordcloud)
docmat<-as.matrix(docs3)
docs3 <- tm_map(docs3,toSpace, "吧")
docmat<-as.matrix(docs3)
docmat<-as.matrix(as.character(docs3))
docmat
View(docmat)
View(docmat)
#wordcloud
c<-as.character(docs3)
library(wordcloud)
c
inspect(docs3)
getwd
library(rJava)
library(magrittr)
library(dplyr)
library(Rfacebook)
library(tmcn)
library(tm)
source('C:/Users/b0520/Desktop/csx_R_course/Rcode/week_4/wordcloud_try1.R', echo=TRUE)
library(Rwordseg)#無法
library(jiebaR)
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
#直接用graph API
prefex="184899118190398"
token ="EAACEdEose0cBAF9HMuGHulkhMKCmw3NbHoeOtuyFxjccgmfTD9ixICZAhZAONemZBzai7QEhWaEtaIodez0ZCZAW6fZAqKl4fiy3WUYND6xlzPsT8j5ZC3jNZB8Wv85OglSHignBSYQIHBQgChxCzFjgDDe7WnsZCXpQQrW30I9FAQuVkhfk0g5xRWb0CrhCPhHEZD"
page<-getPage(prefex,token,n=300)
library(rJava)
library(magrittr)
library(dplyr)
library(Rfacebook)
library(tm)
library(tmcn)
library(Rwordseg)#無法
library(jiebaR)
library(jiebaR)
page<-getPage(prefex,token,n=300)
docs<-Corpus(VectorSource(page$message))
docs<-Corpus(VectorSource(page$message))
#斷詞
cut<-worker()
docs2<-segment(as.character(docs),cut)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "的")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "但")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "而")
docs3 <- tm_map(docs3,toSpace, "吧")
inspect(docs3)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "的")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "但")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "而是")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "吧")
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "就是")
docs3 <- tm_map(docs3,toSpace, "所以")
docs3 <- tm_map(docs3,toSpace, "因為")
docs3 <- tm_map(docs3,toSpace, "因為")
docs3 <- tm_map(docs3,toSpace, "這是")
docs3 <- tm_map(docs3,toSpace, "有")
inspect(docs3)
docs3 <- tm_map(docs3,toSpace, "是")
inspect(docs3)
library(wordcloud)
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
freqFrame = freqFrame[-c(1:34),]
wordcloud(freqFrame$Var1,freqFrame$Freq,
scale=c(5,0.5),min.freq=10,max.words=50,
random.order=FALSE, random.color=TRUE,
rot.per=0, colors=brewer.pal(8, "Dark2"),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
#wordcloud
library(wordcloud2)
install.packages("wordcloud2")
#wordcloud
library(wordcloud2)
wordcloud2(docs3)
as.data.frame(docs3)
as.character(docs3)
tdm <- TermDocumentMatrix(docs3, control = list(wordLengths = c(2, Inf)))
inspect(tdm)
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(2, Inf)))
inspect(tdm)%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
inspect(tdm)%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
d.corpus <- Corpus(VectorSource(docs))
docs3 <- Corpus(VectorSource(docs3))
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "的")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace,"了")
docs3 <- tm_map(docs3,toSpace, "跟")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "但")
docs3 <- tm_map(docs3,toSpace, "不")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "而是")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "在")
docs3 <- tm_map(docs3,toSpace, "那")
docs3 <- tm_map(docs3,toSpace, "吧")
docs3 <- tm_map(docs3,toSpace, "所以")
docs3 <- tm_map(docs3,toSpace, "因為")
docs3 <- tm_map(docs3,toSpace, "這是")
docs3 <- tm_map(docs3,toSpace, "有")
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = v)
df
df<- data.frame(word = names(fre), freq = v)
df<- data.frame(word = names(fre), freq = fre)
View(df)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
docs2<-segment(as.character(docs),cut)
#清洗
docs3<-Corpus(VectorSource(docs2))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
#不用
docs3 <- tm_map(docs3,toSpace, "為")
View(df)
#不用
docs3 <- tm_map(docs3,toSpace, "為")
docs3 <- tm_map(docs3,toSpace, "都")
docs3 <- tm_map(docs3,toSpace,"不")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "讓")
docs3 <- tm_map(docs3,toSpace, "什麼")
docs3 <- tm_map(docs3,toSpace, "對")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "上")
docs3 <- tm_map(docs3,toSpace, "中")
#不用
docs3 <- tm_map(docs3,toSpace, "為")
docs3 <- tm_map(docs3,toSpace, "都")
docs3 <- tm_map(docs3,toSpace,"不")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "讓")
docs3 <- tm_map(docs3,toSpace, "什麼")
docs3 <- tm_map(docs3,toSpace, "對")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "上")
docs3 <- tm_map(docs3,toSpace, "中")
docs3 <- tm_map(docs3,toSpace, "這")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "這樣")
docs3 <- tm_map(docs3,toSpace, "卻")
docs3 <- tm_map(docs3,toSpace, "最")
docs3 <- tm_map(docs3,toSpace, "著")
inspect(docs3)
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
inspect(docs3)
wordcloud2(df)
docs<-Corpus(VectorSource(page$message))
#清洗
docs3<-Corpus(VectorSource(docs))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
#斷詞
cut<-worker()
docs2<-segment(as.character(docs3),cut)
docs3<-Corpus(VectorSource(docs2))
docs4<- Corpus(VectorSource(docs3))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
wordcloud2(df)
View(df)
#清洗
docs3<-Corpus(VectorSource(docs))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
#不用
docs3 <- tm_map(docs3,toSpace, "為")
docs3 <- tm_map(docs3,toSpace, "都")
docs3 <- tm_map(docs3,toSpace,"不")
docs3 <- tm_map(docs3,toSpace,"與")
docs3 <- tm_map(docs3,toSpace, "個")
docs3 <- tm_map(docs3,toSpace, "讓")
docs3 <- tm_map(docs3,toSpace, "什麼")
docs3 <- tm_map(docs3,toSpace, "對")
docs3 <- tm_map(docs3,toSpace, "很")
docs3 <- tm_map(docs3,toSpace, "是")
docs3 <- tm_map(docs3,toSpace, "上")
docs3 <- tm_map(docs3,toSpace, "中")
docs3 <- tm_map(docs3,toSpace, "這")
docs3 <- tm_map(docs3,toSpace, "嗎")
docs3 <- tm_map(docs3,toSpace, "這樣")
docs3 <- tm_map(docs3,toSpace, "卻")
docs3 <- tm_map(docs3,toSpace, "最")
docs<-Corpus(VectorSource(page$message))
#清洗
docs3<-Corpus(VectorSource(docs))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, removePunctuation)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
docs3<- tm_map(docs3, stripWhitespace)
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
#斷詞
cut<-worker()
docs4<-segment(as.character(docs3),cut)
docs4<- Corpus(VectorSource(docs4))
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
#清洗
docs3<-Corpus(VectorSource(docs))
docs3<-tm_map(docs3, function(word) {
gsub("[A-Za-z0-9]", "", word)})
docs3<- tm_map(docs3, removePunctuation)
docs3<- tm_map(docs3, removeNumbers)
docs3<- tm_map(docs3, stripWhitespace)
#覺得停止詞包不好用所以不用
docs3<-tm_map(docs3,removeWords,stopwordsCN())
inspect(docs3)
inspect(docs3)
docs3
docs4<-segment(as.character(docs3),cut)
inspect(docs4)
docs4<- Corpus(VectorSource(docs4))
inspect(docs4)
docmat<- TermDocumentMatrix(docs4, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
wordcloud2(df)
View(d.corpus)
docs<-Corpus(VectorSource(page$message))
#直接用graph API
prefex="184899118190398"
token ="EAACEdEose0cBADfqsgPWGo8HES8chDIbP94R74lrmgPpvfLu6VGOFvqIkFZBDg0OcOA8bDjnPG9LoaQG8YG7E02GG2OUDRqLcQmGI1PAFrxMifxlaU5BZCOuhLcOAACmZAg5bhCrz3ifWfHDkUKDuzC4InYhLgpQhehUYpvcIbUAECB15tHsflWV9fU0tYZD"
library(rJava)
library(magrittr)
library(dplyr)
library(Rfacebook)
library(tm)
library(tmcn)
library(Rwordseg)#無法
library(jiebaR)
library(magrittr)
library(dplyr)
library(Rfacebook)
library(tm)
library(tmcn)
library(Rwordseg)#無法
library(jiebaR)
page<-getPage(prefex,token,n=300)
docs<-Corpus(VectorSource(page$message))
#斷詞
cut<-worker()
docs2<-segment(as.character(docs),cut)
docs3<-Corpus(VectorSource(docs2))
#清洗
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
inspect(docs3)
docs3<-tm_map(docs3, toSpace, "[A-Za-z0-9]")
inspect(docs3)
docs3<-tm_map(docs3, removePunctuation)
docs3<-tm_map(docs3, removeNumbers)
docs3<-tm_map(docs3, stripWhitespace)
inspect(docs3)
#覺得停止詞包不好用所以不用?
docs3<-tm_map(docs3,removeWords,stopwordsCN())
inspect(docs3)
Sys.setlocale(category='LC_ALL', locale='C')
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(2, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
inspect(docs3)
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(2, Inf),encoding="UTF-8")))%>%as.matrix()
?TermDocumentMatrix
class(docs3)
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(1, Inf),))%>%as.matrix()
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(1, Inf),))%>%as.matrix()
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(1, Inf)))%>%as.matrix()
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
View(df)
原文網址：https://read01.com/P8D5Qz.html))%>%as.matrix()
docmat<- TermDocumentMatrix(docs3, control = list(wordLengths = c(1, Inf)))
docmat<-as.matrix(docmat)
View(docmat)
fre <- sort(rowSums(docmat), decreasing = TRUE)
df<- data.frame(word = names(fre), freq = fre)
df
Encoding(docs3)  <- "UTF-8"
Encoding(page$message)  <- "UTF-8"
docs<-Corpus(VectorSource(page$message))
#斷詞
cut<-worker()
docs2<-segment(as.character(docs),cut)
docs3<-Corpus(VectorSource(docs2))
#清洗
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))}
)
inspect(docs3)
docs3<-tm_map(docs3, toSpace, "[A-Za-z0-9]")
